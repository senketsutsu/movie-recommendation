{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    #embedding computation\n",
    "    'cleora_n_iter': 3,\n",
    "    'cleora_dim': 512,\n",
    "    \n",
    "    #dataset preparation\n",
    "    'train_test_split': 0.2,\n",
    "    \n",
    "    'batch_size': 256,\n",
    "    'test_batch_size': 500,\n",
    "    'epochs': [3],\n",
    "    'alpha': [1e-4],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the Facebook dataset from SNAP: https://snap.stanford.edu/data/facebook-large-page-page-network.html ->  \n",
    "```bash\n",
    "wget https://snap.stanford.edu/data/facebook_large.zip\n",
    "```\n",
    "2. Extract the dataset to ./data/facebook_large/\n",
    "\n",
    "Other datasets from SNAP can be preprocessed similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/facebook_large/musae_facebook_edges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>21708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>22208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>22171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_1   id_2\n",
       "0     0  18427\n",
       "1     1  21708\n",
       "2     1  22208\n",
       "3     1  22171\n",
       "4     1   6829"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=config['train_test_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144023</th>\n",
       "      <td>13582</td>\n",
       "      <td>17833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137388</th>\n",
       "      <td>8090</td>\n",
       "      <td>16502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128146</th>\n",
       "      <td>11323</td>\n",
       "      <td>20700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92044</th>\n",
       "      <td>7263</td>\n",
       "      <td>16965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>16260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id_1   id_2\n",
       "144023  13582  17833\n",
       "137388   8090  16502\n",
       "128146  11323  20700\n",
       "92044    7263  16965\n",
       "19          1  16260"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136801, 2), (34201, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_cleora_input_clique_filename = \"../fb/fb_cleora_input_clique.txt\"\n",
    "fb_cleora_input_star_filename = \"../fb/fb_cleora_input_star.txt\"\n",
    "fb_lp_train_filename = \"../fb/fb_lp_train.txt\"\n",
    "fb_lp_test_filename = \"../fb/fb_lp_test.txt\"\n",
    "output_dir = '../output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fb_cleora_input_clique_filename, \"w\") as f_cleora_clique, open(fb_cleora_input_star_filename, \"w\") as f_cleora_star, open(fb_lp_train_filename, \"w\") as f_train:\n",
    "    grouped_train = train.groupby('id_1')\n",
    "    for n, (name, group) in enumerate(grouped_train):\n",
    "        group_list = group['id_2'].tolist()\n",
    "        group_elems = list(map(str, group_list))\n",
    "        f_cleora_clique.write(\"{} {}\\n\".format(name, ' '.join(group_elems)))\n",
    "        f_cleora_star.write(\"{}\\t{}\\n\".format(n, name))\n",
    "        for elem in group_elems:\n",
    "            f_train.write(\"{}\\t{}\\n\".format(name, elem))\n",
    "            f_cleora_star.write(\"{}\\t{}\\n\".format(n, elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fb_lp_test_filename, \"w\") as f_test:\n",
    "    grouped_test = test.groupby('id_1')\n",
    "    for name, group in grouped_test:\n",
    "        group_list = group['id_2'].tolist()\n",
    "        group_elems = list(map(str, group_list))\n",
    "        for elem in group_elems:\n",
    "            f_test.write(\"{}\\t{}\\n\".format(name, elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleora training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download an appropriate binary Cleora release from: https://github.com/Synerise/cleora/releases . \n",
    "\n",
    "A Linux GNU version is assumed in this example, but any other will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEORA_BINARY = '../src/cleora-v1.2.3-x86_64-unknown-linux-gnu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def columns2output_filename(output_dir, columns):\n",
    "    columns_split = columns.split()\n",
    "    if len(columns_split) == 1 and 'reflexive' in columns:\n",
    "        column_name = columns.split('::')[-1]\n",
    "        return os.path.join(output_dir, f'emb__{column_name}__{column_name}.out')\n",
    "\n",
    "    column_names = [i.split('::')[-1] for i in columns_split]\n",
    "    return os.path.join(output_dir, 'emb__' + '__'.join(column_names) + '.out')\n",
    "\n",
    "\n",
    "def train_cleora(dim, n_iter, columns, input_filename, output_dir):\n",
    "    command = [CLEORA_BINARY,\n",
    "                '--columns', columns,\n",
    "                '--dimension', str(dim), \n",
    "                '-n', str(n_iter), \n",
    "                '--input', input_filename, \n",
    "                '-o', output_dir]\n",
    "    subprocess.run(command, check=True)\n",
    "    return columns2output_filename(output_dir, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star expansion\n",
    "\n",
    "In the `fb_cleora_input_star.txt` file the first column is a virtual node. The parameter `-c \"transient::cluster_id node\"` means that embeddings will not be created for nodes from this column. This translates to star expansion scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora\u001b[0m\u001b[38;5;8m]\u001b[0m Reading args...\n",
      "[src/main.rs:222] &config = Configuration {\n",
      "    produce_entity_occurrence_count: true,\n",
      "    embeddings_dimension: 512,\n",
      "    max_number_of_iteration: 3,\n",
      "    seed: None,\n",
      "    prepend_field: false,\n",
      "    log_every_n: 10000,\n",
      "    in_memory_embedding_calculation: true,\n",
      "    input: [\n",
      "        \"../fb/fb_cleora_input_star.txt\",\n",
      "    ],\n",
      "    file_type: Tsv,\n",
      "    output_dir: Some(\n",
      "        \"../output\",\n",
      "    ),\n",
      "    output_format: TextFile,\n",
      "    relation_name: \"emb\",\n",
      "    columns: [\n",
      "        Column {\n",
      "            name: \"cluster_id\",\n",
      "            transient: true,\n",
      "            complex: false,\n",
      "            reflexive: false,\n",
      "            ignored: false,\n",
      "        },\n",
      "        Column {\n",
      "            name: \"StarNode\",\n",
      "            transient: false,\n",
      "            complex: false,\n",
      "            reflexive: false,\n",
      "            ignored: false,\n",
      "        },\n",
      "    ],\n",
      "}\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora\u001b[0m\u001b[38;5;8m]\u001b[0m Starting calculation...\n",
      "[src/pipeline.rs:25] &sparse_matrices = [\n",
      "    SparseMatrix {\n",
      "        col_a_id: 0,\n",
      "        col_a_name: \"cluster_id\",\n",
      "        col_b_id: 1,\n",
      "        col_b_name: \"StarNode\",\n",
      "        edge_count: 0,\n",
      "        hash_2_id: {},\n",
      "        id_2_hash: [],\n",
      "        row_sum: [],\n",
      "        pair_index: {},\n",
      "        entries: [],\n",
      "    },\n",
      "]\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 10000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 20000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 30000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 40000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 50000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 60000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 70000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 80000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 90000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 100000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 110000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 120000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:02Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 130000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 140000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 150000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::sparse_matrix\u001b[0m\u001b[38;5;8m]\u001b[0m Number of entities: 39361\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::sparse_matrix\u001b[0m\u001b[38;5;8m]\u001b[0m Number of edges: 154319\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::sparse_matrix\u001b[0m\u001b[38;5;8m]\u001b[0m Number of entries: 308356\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::sparse_matrix\u001b[0m\u001b[38;5;8m]\u001b[0m Total memory usage by the struct ~ 13 MB\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora\u001b[0m\u001b[38;5;8m]\u001b[0m Finished Sparse Matrices calculation in 0 sec\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Start initialization. Dims: 512, entities: 39361.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done initializing. Dims: 512, entities: 39361.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Start propagating. Number of iterations: 3.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done iter: 0. Dims: 512, entities: 39361, num data points: 308356.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done iter: 1. Dims: 512, entities: 39361, num data points: 308356.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done iter: 2. Dims: 512, entities: 39361, num data points: 308356.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done propagating.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:03Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Start saving embeddings.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:04Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done saving embeddings.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:04Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Finalizing embeddings calculations!\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:04Z \u001b[0m\u001b[32mINFO \u001b[0m cleora\u001b[0m\u001b[38;5;8m]\u001b[0m Finished in 2 sec\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "cleora_output_star_filename = train_cleora(config['cleora_dim'], config['cleora_n_iter'], \"transient::cluster_id StarNode\", fb_cleora_input_star_filename, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clique expansion\n",
    "\n",
    "The `fb_cleora_input_clique.txt` file has the structure of adjacency list. The parameter `-c \"complex::reflexive::node\"` means that edges will be created for all cominations of nodes from each line. This translates to clique expansion scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:04Z \u001b[0m\u001b[32mINFO \u001b[0m cleora\u001b[0m\u001b[38;5;8m]\u001b[0m Reading args...\n",
      "[src/main.rs:222] &config = Configuration {\n",
      "    produce_entity_occurrence_count: true,\n",
      "    embeddings_dimension: 512,\n",
      "    max_number_of_iteration: 3,\n",
      "    seed: None,\n",
      "    prepend_field: false,\n",
      "    log_every_n: 10000,\n",
      "    in_memory_embedding_calculation: true,\n",
      "    input: [\n",
      "        \"../fb/fb_cleora_input_clique.txt\",\n",
      "    ],\n",
      "    file_type: Tsv,\n",
      "    output_dir: Some(\n",
      "        \"../output\",\n",
      "    ),\n",
      "    output_format: TextFile,\n",
      "    relation_name: \"emb\",\n",
      "    columns: [\n",
      "        Column {\n",
      "            name: \"CliqueNode\",\n",
      "            transient: false,\n",
      "            complex: true,\n",
      "            reflexive: true,\n",
      "            ignored: false,\n",
      "        },\n",
      "    ],\n",
      "}\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:04Z \u001b[0m\u001b[32mINFO \u001b[0m cleora\u001b[0m\u001b[38;5;8m]\u001b[0m Starting calculation...\n",
      "[src/pipeline.rs:25] &sparse_matrices = [\n",
      "    SparseMatrix {\n",
      "        col_a_id: 0,\n",
      "        col_a_name: \"CliqueNode\",\n",
      "        col_b_id: 1,\n",
      "        col_b_name: \"CliqueNode\",\n",
      "        edge_count: 0,\n",
      "        hash_2_id: {},\n",
      "        id_2_hash: [],\n",
      "        row_sum: [],\n",
      "        pair_index: {},\n",
      "        entries: [],\n",
      "    },\n",
      "]\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:06Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::pipeline\u001b[0m\u001b[38;5;8m]\u001b[0m Number of lines processed: 10000\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:07Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::sparse_matrix\u001b[0m\u001b[38;5;8m]\u001b[0m Number of entities: 21843\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:07Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::sparse_matrix\u001b[0m\u001b[38;5;8m]\u001b[0m Number of edges: 4510583\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:07Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::sparse_matrix\u001b[0m\u001b[38;5;8m]\u001b[0m Number of entries: 2220567\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:07Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::sparse_matrix\u001b[0m\u001b[38;5;8m]\u001b[0m Total memory usage by the struct ~ 90 MB\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:07Z \u001b[0m\u001b[32mINFO \u001b[0m cleora\u001b[0m\u001b[38;5;8m]\u001b[0m Finished Sparse Matrices calculation in 2 sec\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:07Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Start initialization. Dims: 512, entities: 21843.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:07Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done initializing. Dims: 512, entities: 21843.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:07Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Start propagating. Number of iterations: 3.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:08Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done iter: 0. Dims: 512, entities: 21843, num data points: 2220567.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:08Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done iter: 1. Dims: 512, entities: 21843, num data points: 2220567.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:09Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done iter: 2. Dims: 512, entities: 21843, num data points: 2220567.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:09Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done propagating.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:09Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Start saving embeddings.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:10Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Done saving embeddings.\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:10Z \u001b[0m\u001b[32mINFO \u001b[0m cleora::embedding\u001b[0m\u001b[38;5;8m]\u001b[0m Finalizing embeddings calculations!\n",
      "\u001b[0m\u001b[38;5;8m[\u001b[0m2024-11-17T12:27:10Z \u001b[0m\u001b[32mINFO \u001b[0m cleora\u001b[0m\u001b[38;5;8m]\u001b[0m Finished in 5 sec\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "cleora_output_clique_filename = train_cleora(config['cleora_dim'], config['cleora_n_iter'], \"complex::reflexive::CliqueNode\", fb_cleora_input_clique_filename, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No expansion\n",
    "\n",
    "You can also compute Cleora without any expansion scheme by providing an input file in the edgelist format (single pair of nodes per line). Run with a simple parameter: `-c \"node1 node2\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction\n",
    "\n",
    "In the link prediction task, we train a binary classifier to distinguish real edges from fake edges. Real edges are composed of pairs of nodes from train/test set, while fake edges are built in two ways depending on whether we're training or testing:\n",
    "\n",
    "1. In training: we draw random pairs of edges\n",
    "2. In testing: we take a valid pair of nodes (nodeA-nodeB) from the testset. Then we pair nodeA to 10.000 most common nodes in the dataset. This way, we obtain 1 real and 10.000 fake examples.\n",
    "\n",
    "We compute a Hadamard product between pairs of embeddings. As a result, we obtain a single vector for each embedding pair, which represents an approximation of node similarity. We feed the products as inputs to the classifier.\n",
    "\n",
    "At test time, we compute ranking metrics of the correct prediction among the 10.001 given pairs of nodes. We compute the mean reciprocal rank measure (MRR) and hit rate in top 10 predictins (HR@10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(input_file):\n",
    "    df_full = pd.read_csv(input_file, delimiter = \" \", skiprows=[0], header=None, \n",
    "                     index_col=0)\n",
    "    df_full = df_full.drop([1], axis=1)  \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test(embeddings):\n",
    "    valid_idx = embeddings.index.to_numpy()\n",
    "    train = np.loadtxt(fb_lp_train_filename, delimiter=\"\\t\", dtype=np.int32)\n",
    "    test = np.loadtxt(fb_lp_test_filename, delimiter=\"\\t\", dtype=np.int32)\n",
    "    \n",
    "    #valid pairs of nodes\n",
    "    train = train[np.isin(train[:,0], valid_idx) & np.isin(train[:,1], valid_idx)]\n",
    "    test = test[np.isin(test[:,0], valid_idx) & np.isin(test[:,1], valid_idx)]\n",
    "    \n",
    "    #negatives for testset: top 10000 most common nodes\n",
    "    all_idx = train.flatten()\n",
    "    ctr = Counter(all_idx)\n",
    "    negatives = ctr.most_common(10000)\n",
    "    negatives = [ n[0] for n in negatives ]\n",
    "    \n",
    "    adjacency_dict = dict()\n",
    "    for inp, out in np.vstack([test, train]):\n",
    "        if inp not in adjacency_dict:\n",
    "            adjacency_dict[inp] = set()\n",
    "        adjacency_dict[inp].add(out)\n",
    "\n",
    "    return train, test, negatives, adjacency_dict, valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config['batch_size']\n",
    "test_batch_size = config['test_batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 535/535 [00:10<00:00, 49.60it/s]\n",
      " 59%|█████▉    | 318/535 [00:06<00:04, 49.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,train_1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],batch_size)):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#ones = real pairs of nodes\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#zeros = fake pairs of nodes\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     ones\u001b[38;5;241m=\u001b[39mtrain_1[idx:\u001b[38;5;28mmin\u001b[39m(idx\u001b[38;5;241m+\u001b[39mbatch_size,train_1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),:]\n\u001b[0;32m---> 23\u001b[0m     ones_emb_in \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mones\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     24\u001b[0m     ones_emb_out \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mloc[ones[:,\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#Hadamard\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/movie-recommendation-KK-5LRNj/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/movie-recommendation-KK-5LRNj/lib/python3.12/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/movie-recommendation-KK-5LRNj/lib/python3.12/site-packages/pandas/core/indexing.py:1361\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[0;32m-> 1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_with_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   1363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/movie-recommendation-KK-5LRNj/lib/python3.12/site-packages/pandas/core/generic.py:5686\u001b[0m, in \u001b[0;36mNDFrame._reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   5683\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m ensure_platform_int(indexer)\n\u001b[1;32m   5685\u001b[0m \u001b[38;5;66;03m# TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)\u001b[39;00m\n\u001b[0;32m-> 5686\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5689\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_dups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5693\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5694\u001b[0m \u001b[38;5;66;03m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[1;32m   5695\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/movie-recommendation-KK-5LRNj/lib/python3.12/site-packages/pandas/core/internals/managers.py:688\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[1;32m    681\u001b[0m         indexer,\n\u001b[1;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[1;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 688\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    696\u001b[0m     ]\n\u001b[1;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/movie-recommendation-KK-5LRNj/lib/python3.12/site-packages/pandas/core/internals/blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/movie-recommendation-KK-5LRNj/lib/python3.12/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/movie-recommendation-KK-5LRNj/lib/python3.12/site-packages/pandas/core/array_algos/take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    161\u001b[0m )\n\u001b[0;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[1;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for algo in [cleora_output_star_filename, cleora_output_clique_filename]:\n",
    "    embeddings = read_embeddings(algo)\n",
    "    train_1, test_1, negatives, adjacency_dict, valid_idx = read_train_test(embeddings)\n",
    "    #for faster operation, draw only 1000 test examples\n",
    "    test_ex = random.sample(list(test_1), 1000)\n",
    "    \n",
    "    #these are the 10.000 most common nodes selected as negatives for each valid testing node pair\n",
    "    df_neg = embeddings.loc[negatives]\n",
    "    neg_ids = set(df_neg.index)\n",
    "\n",
    "    epoch = max(config['epochs'])\n",
    "    for a in config['alpha']:\n",
    "        #create a binary classifier outputting whether a node pair represents a valid edge (1) or not a valid edge (0)\n",
    "        clf = SGDClassifier(random_state=0, loss='log_loss', alpha=a)\n",
    "        for e in range(0, epoch):\n",
    "            np.random.shuffle(train_1)\n",
    "            \n",
    "            for idx in tqdm(range(0,train_1.shape[0],batch_size)):\n",
    "                #ones = real pairs of nodes\n",
    "                #zeros = fake pairs of nodes\n",
    "                ones=train_1[idx:min(idx+batch_size,train_1.shape[0]),:]\n",
    "                \n",
    "                ones_emb_in = embeddings.loc[ones[:,0]].to_numpy()\n",
    "                ones_emb_out = embeddings.loc[ones[:,1]].to_numpy()\n",
    "                #Hadamard\n",
    "                ones = np.multiply(ones_emb_in,ones_emb_out)\n",
    "                \n",
    "                id_train_0_in = np.random.choice(valid_idx, size=len(ones), replace=True)\n",
    "                id_train_0_out = np.random.choice(valid_idx, size=len(ones), replace=True)\n",
    "    \n",
    "                zeros_emb_in = embeddings.loc[id_train_0_in].to_numpy()\n",
    "                zeros_emb_out = embeddings.loc[id_train_0_out].to_numpy()\n",
    "                #Hadamard\n",
    "                zeros = np.multiply(zeros_emb_in, zeros_emb_out)\n",
    "    \n",
    "                x_train = np.vstack([ones, zeros])\n",
    "                y_train = [1]*len(ones) + [0]*len(ones)\n",
    "\n",
    "                clf.partial_fit(x_train, y_train, classes=[0,1])\n",
    "\n",
    "            if e+1 in config['epochs']:\n",
    "                mrr = 0.0\n",
    "                hr = 0.0\n",
    "                for n, ex in enumerate(test_ex):\n",
    "                    l = ex[0]\n",
    "                    r = ex[1]\n",
    "\n",
    "                    emb_l = embeddings.loc[l].to_numpy().reshape([1, -1])\n",
    "                    emb_r = np.vstack((df_neg.to_numpy(), embeddings.loc[r].to_numpy()))\n",
    "        \n",
    "                    full_ex = np.hstack([np.repeat(emb_l, len(emb_r), axis=0), emb_r])\n",
    "                    hadamard = np.multiply(emb_l, emb_r)\n",
    "                    preds = clf.predict_proba(hadamard)[:,1]\n",
    "                    preds = np.array(preds)\n",
    "\n",
    "                    #do not punish for high scores of items from trainset and others from testset\n",
    "                    forbidden_ex = adjacency_dict[l]\n",
    "                    df_mask = [0 if (elem in forbidden_ex) else 1 for elem in neg_ids]\n",
    "                    #last elem is always valid\n",
    "                    df_mask.append(1)\n",
    "                    preds *= df_mask\n",
    "            \n",
    "                    ranking = (-preds).argsort()\n",
    "                    rank = np.isin(ranking, 10000).nonzero()[0][0]+1\n",
    "                    mrr += 1/rank\n",
    "                    hr += (rank <= 10)\n",
    "                    \n",
    "                    if (n+1)%100 == 0:\n",
    "                        print('mrr ', mrr/(n+1), ' hr@10 ', hr/(n+1))\n",
    "\n",
    "                print('algo: {} epochs: {} lr: {}, mrr: {}, hr@10: {}'.format(algo, str(e+1), a, mrr/len(test_ex), hr/len(test_ex)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie-recommendation-KK-5LRNj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
